{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 27.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: nltk>=3.1 in /home/ec2-user/anaconda3/envs/chainer_p36/lib/python3.6/site-packages (from textblob) (3.3)\n",
      "Requirement not upgraded as not directly required: six in /home/ec2-user/anaconda3/envs/chainer_p36/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.14.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,FloatType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "!pip install -U textblob\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-57-116.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f936ef298d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get my execution role as defined based on my IAM policy\n",
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath).master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# start SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------------+---------------+--------------------+\n",
      "|         twitter_id|         name|         created_at|followers_count|                text|\n",
      "+-------------------+-------------+-------------------+---------------+--------------------+\n",
      "|1248037911578148866|     OnTopMag|2020-04-09 00:00:00|           5038|'Fauci: Pandemic ...|\n",
      "|1248037911532113928|  RedMaryland|2020-04-09 00:00:00|           5062|'Red Maryland is ...|\n",
      "|1248037911557218304|   PEDro_CEBP|2020-04-09 00:00:00|           9659|'MindSpot has rel...|\n",
      "|1248037911582232576|CheckPointOrg|2020-04-09 00:00:00|           5379|'Staying social o...|\n",
      "|1248037911921963008|salvationarmy|2020-04-09 00:00:00|          63374|'The Salvation Ar...|\n",
      "+-------------------+-------------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.{}.amazonaws.com'.format(region))\n",
    "\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true',inferschema='true').\\\n",
    "                                load('s3a://twitter-bucket-jingyusu/stream Covid-19 4-09.csv')\n",
    "drop_list = ['place']\n",
    "df = df.select([column for column in df.columns if column not in drop_list])\n",
    "df = df.dropna()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@[A-Za-z0-9_]+|https?://[^ ]+|^.RT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "pat3 = r'^.RT'\n",
    "hashtag = r'\\#\\S*'\n",
    "combined_pat = r'|'.join((pat1, pat2, pat3))\n",
    "print(combined_pat)\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z19]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "text_process_udf = udf(tweet_cleaner_updated, StringType())\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())\n",
    "\n",
    "def condition(r):\n",
    "    if (r >=0.1):\n",
    "        label = \"positive\"\n",
    "    elif(r <= -0.1):\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "    return label\n",
    "\n",
    "sentiment_udf = udf(lambda x: condition(x), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------------+---------------+--------------------+--------------------+---------------+---------+\n",
      "|         twitter_id|         name|         created_at|followers_count|                text|      text_processed|sentiment_score|sentiment|\n",
      "+-------------------+-------------+-------------------+---------------+--------------------+--------------------+---------------+---------+\n",
      "|1248037911578148866|     OnTopMag|2020-04-09 00:00:00|           5038|'Fauci: Pandemic ...|fauci pandemic ha...|     0.40833333| positive|\n",
      "|1248037911532113928|  RedMaryland|2020-04-09 00:00:00|           5062|'Red Maryland is ...|red maryland is l...|     0.06818182|  neutral|\n",
      "|1248037911557218304|   PEDro_CEBP|2020-04-09 00:00:00|           9659|'MindSpot has rel...|mindspot has rele...|    0.016666668|  neutral|\n",
      "|1248037911582232576|CheckPointOrg|2020-04-09 00:00:00|           5379|'Staying social o...|staying social on...|     0.24242425| positive|\n",
      "|1248037911921963008|salvationarmy|2020-04-09 00:00:00|          63374|'The Salvation Ar...|the salvation arm...|            0.0|  neutral|\n",
      "+-------------------+-------------+-------------------+---------------+--------------------+--------------------+---------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process the test to analyis the sentiment.\n",
    "df = df.withColumn('text_processed',text_process_udf(df['text']))\n",
    "df  = df.withColumn(\"sentiment_score\", sentiment_analysis_udf( df['text_processed'] ))\n",
    "df  = df.withColumn(\"sentiment\", sentiment_udf( df['sentiment_score'] ))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.436854551114363, 50.657804914012964, 36.90534053487267]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis the sentiment\n",
    "grouped = df.groupby(\"sentiment\").count()\n",
    "result = grouped.withColumn('percent', (grouped['count']/df.count()) * 100).orderBy(\"sentiment\")\n",
    "a = result.select('percent').collect()\n",
    "ans = [i[0] for i in a]\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionize the code\n",
    "def get_sentiemnt(file):\n",
    "    df = spark.read.format('com.databricks.spark.csv').\\\n",
    "            options(header='true',inferschema='true').\\\n",
    "            load(file)\n",
    "    drop_list = ['place']\n",
    "    df = df.select([column for column in df.columns if column not in drop_list])\n",
    "    df = df.dropna()\n",
    "    df = df.withColumn('text_processed',text_process_udf(df['text']))\n",
    "    df  = df.withColumn(\"sentiment_score\", sentiment_analysis_udf( df['text_processed'] ))\n",
    "    df  = df.withColumn(\"sentiment\", sentiment_udf( df['sentiment_score'] ))\n",
    "    grouped = df.groupby(\"sentiment\").count()\n",
    "    result = grouped.withColumn('percent', (grouped['count']/df.count()) * 100).orderBy(\"sentiment\")\n",
    "    ans = result.select('percent').collect()\n",
    "    return [i[0] for i in ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.Session().resource('s3')\n",
    "your_bucket = s3.Bucket('twitter-bucket-jingyusu')\n",
    "\n",
    "filelist = []\n",
    "for s3_file in your_bucket.objects.all():\n",
    "    if 'tweets' not in s3_file.key:\n",
    "        filelist.append(s3_file.key)\n",
    "filelist.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stream Covid-19 4-08.csv',\n",
       " 'stream Covid-19 4-09.csv',\n",
       " 'stream Covid-19 4-12.csv',\n",
       " 'stream Covid-19 4-13.csv',\n",
       " 'stream Covid-19 4-14.csv',\n",
       " 'stream Covid-19 4-15.csv',\n",
       " 'stream Covid-19 4-16.csv',\n",
       " 'stream Covid-19 4-17.csv',\n",
       " 'stream Covid-19 4-18.csv',\n",
       " 'stream Covid-19 4-19.csv',\n",
       " 'stream Covid-19 4-20.csv',\n",
       " 'stream Covid-19 4-21.csv',\n",
       " 'stream Covid-19 4-22.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "result_list = []\n",
    "for file in filelist:\n",
    "    file_name = 's3a://twitter-bucket-jingyusu/' + file\n",
    "    result_list.append(get_sentiemnt(file_name))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))# main loop to process all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(result_list)), [i[0] for i in result_list],label='negative ')\n",
    "plt.plot(np.arange(len(result_list)), [i[1] for i in result_list],label='neutral')\n",
    "plt.plot(np.arange(len(result_list)), [i[2] for i in result_list],label='positive')\n",
    "ax.set_xticklabels([i.split(' ')[-1].split('.')[0] for i in filelist])\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
